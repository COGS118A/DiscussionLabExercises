[{'question': '\n    Recap: When doing linear regression using least squares estimation, what is the analytical solution\n    for the $W^{*}$ \n    ', 'type': 'multiple_choice', 'question_id': 'A1', 'answers': [{'answer': '$\\left(XY^T\\right)^{-1}XY^T$', 'id': 1}, {'answer': '$\\left(X^TY\\right)^{-1}XY$', 'id': 2}, {'answer': '$\\left(X^TX\\right)^{-1}X^TY$', 'id': 3}, {'answer': '$\\left(Y^TX^T\\right)^{-1}XY^T$', 'id': 4}]}, {'question': '\n    Compared with OLS regression which can compute $\\vec{W}^*$ analytically, \n    why do we use gradient descent sometime instead?\n    ', 'type': 'multiple_choice', 'question_id': 'A2', 'answers': [{'answer': 'Because $\\vec{w}$ can be calculated analytically in gradient descent', 'id': 1}, {'answer': 'Because some loss functions (such as $L_1$) are not convex', 'id': 2}, {'answer': 'Because gradient descent will yield more accurate result than OLS', 'id': 3}, {'answer': 'Because gradient descent will sometimes get stuck in local minima', 'id': 4}]}, {'question': '\n    When Using $L_1$ as our penalty metric, what is the Loss (Cost) Function? $L(W) =$...\n    ', 'type': 'multiple_choice', 'question_id': 'A3', 'answers': [{'answer': '$\\sum^n_{i=1}\\left| \\mathbf{x}^T_iW+y_i \\right|$', 'id': 1}, {'answer': '$\\sum^n_{i=1}\\mathbf{x}^T_iW-y_i$', 'id': 2}, {'answer': '$\\sum^n_{i=1}\\left( \\mathbf{x}^T_iW-y_i \\right)^2$', 'id': 3}, {'answer': '$\\sum^n_{i=1}\\left| \\mathbf{x}^T_iW-y_i \\right|$', 'id': 4}]}, {'question': '\n    Compared with $L_2$ norm penalty, what advantage do the $L_1$ norm have in optimization question? Why?\n    ', 'type': 'multiple_choice', 'question_id': 'A4', 'answers': [{'answer': '$L_2$ norm is less sensitive to the outliers. <br> $L_1$ norm amplifies the outlier by the squared term', 'id': 1}, {'answer': '$L_1$ norm is less sensitive to the outliers. <br> $L_2$ norm amplifies the outlier by the squared term', 'id': 2}, {'answer': '$L_1$ norm is more easily computed. <br> $L_2$ take longer to compute because of thesquared term', 'id': 3}, {'answer': '$L_2$ norm is more easily computed. <br> $L_1$ take longer to compute because of thesquared term', 'id': 4}]}, {'question': '\n    When Using $L_1$ as our penalty metric, what is the Gradient of Loss (Cost) Function? \n    $\\frac{\\partial L(W)}{\\partial W} = $...\n    ', 'type': 'multiple_choice', 'question_id': 'A5', 'answers': [{'answer': '$\\sum^n_{i=1}\\left( \\mathbf{x}^{-1}_iW-y_i \\right) \\times \\mathbf{x}_i$', 'id': 1}, {'answer': '$\\sum^n_{i=1}-\\text{sign}\\left( \\mathbf{x}^T_iW-y_i \\right) \\times \\mathbf{x}_i$', 'id': 2}, {'answer': '$\\sum^n_{i=1}\\text{sign}\\left( \\mathbf{x}^T_iW-y_i \\right) \\times \\mathbf{x}_i$', 'id': 3}, {'answer': '$\\sum^n_{i=1} \\text{sign}\\left( \\mathbf{x}^T_iW \\right) \\times \\mathbf{x}_i$', 'id': 4}]}, {'question': '\n    After we obtained the gradient of our loss function, we do the gradient descent with this formula\n    $W_{t+1} = W_t - \\lambda_t  \\frac{\\partial L(W)}{\\partial W}$. What is the interpretation of $\\lambda_t$\n    and changes to the $W_{t+1}$ in each iteration?\n    ', 'type': 'multiple_choice', 'question_id': 'A6', 'answers': [{'answer': 'After each iteration, we modify the weight vector in the direction of the negative gradient', 'id': 1}, {'answer': 'After each iteration, we modify the weight vector in the direction of the gradient', 'id': 2}, {'answer': 'After each iteration, we modify the weight vector to approach $0$', 'id': 3}, {'answer': 'After each iteration, the weight vector approach closer to $\\inf$', 'id': 4}]}, {'question': '\n    After we obtained the gradient of our loss function, we do the gradient descent with this formula\n    $W_{t+1} = W_t - \\lambda_t  \\frac{\\partial L(W)}{\\partial W}$. In the gradient descent algorithm \n    each update of the weight vector _________.\n    ', 'type': 'multiple_choice', 'question_id': 'A7', 'answers': [{'answer': 'only depends on training examples that are far away from the regression line', 'id': 1}, {'answer': 'not depends on all the training examples', 'id': 2}, {'answer': 'depends on only correctly classified points.', 'id': 3}, {'answer': 'depends on all the training examples.', 'id': 4}]}]